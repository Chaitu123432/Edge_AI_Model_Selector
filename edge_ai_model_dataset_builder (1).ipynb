{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d8e98d",
   "metadata": {},
   "source": [
    "# üìò Edge AI Model Dataset Builder\n",
    "This notebook benchmarks multiple AI models (image, audio, NLP) on your device, builds a latency/memory dataset, trains a surrogate model, and selects the optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil pynvml onnxruntime torch torchvision numpy pandas scikit-learn transformers requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd71f51",
   "metadata": {},
   "source": [
    "## üß© Step 1: Probe Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da82b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edge_selector.probe import DeviceProbe\n",
    "import json\n",
    "profile = DeviceProbe().run()\n",
    "json.dump(profile, open('device_profile.json','w'), indent=2)\n",
    "print('‚úÖ Device profile saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f7f46",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Benchmark All Registered Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_all_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c3fee",
   "metadata": {},
   "source": [
    "## üìä Step 3: Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "rows = [json.loads(l) for l in open('runs.ndjson')]\n",
    "df = pd.DataFrame(rows)\n",
    "display(df[['meta','framework','latency_ms','mem_mb']].head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94693aa2",
   "metadata": {},
   "source": [
    "## üß† Step 4: Train Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edge_selector.surrogate import Surrogate\n",
    "s = Surrogate()\n",
    "s.train('runs.ndjson')\n",
    "print('‚úÖ Surrogate trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8628ec2",
   "metadata": {},
   "source": [
    "## üßÆ Step 5: Select Best Model Based on SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from edge_selector.selector import Selector\n",
    "context = json.load(open('device_profile.json'))\n",
    "actions = [{'model': m, 'params': 1e6, 'backend': 'torch', 'est_quality': 0.85} for m in df['meta'].apply(lambda x: x['model_name']).unique()]\n",
    "sel = Selector(s)\n",
    "print(sel.select(context, actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60326470",
   "metadata": {},
   "source": [
    "## üìà Step 6: Visualize Model Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df['latency_ms'], df['mem_mb'])\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Model Latency vs Memory Usage')\n",
    "for i,m in enumerate(df['meta'].apply(lambda x: x['model_name'])):\n",
    "    plt.text(df['latency_ms'].iloc[i]+1, df['mem_mb'].iloc[i], m, fontsize=8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
